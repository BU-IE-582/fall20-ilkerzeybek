---
title: "Homework 3"
author: "İlker Zeybek"
date: "January 1, 2021"
output: html_document
---

# Introduction

In this homework, we are interested in predicting the tomorrow’s hourly electricity consumption of Turkey.
The consumption series are made publicly available by EPİAŞ at this [link](https://seffaflik.epias.com.tr/transparency/.
). Our main task will be devising a penalized regression approach to forecast the next-day’s hourly
consumption. In other words, we are expected to provide 24 predictions corresponding to the hours of
the next day.

# Initial Data Manipulation

Firstly, I have loaded the necessary packages for the future tasks and set the seed for the reproducibility of my work.

```{r warning = FALSE, message = FALSE, cache = TRUE}
library(lubridate)
library(dplyr)
library(tidyr)
library(MLmetrics)
library(glmnet)
library(ggplot2)
library(plotly)
library(htmltools)
set.seed(582)
```

Then I have loaded the data set acquired from [EPİAŞ](https://seffaflik.epias.com.tr/transparency/tuketim/gerceklesen-tuketim/gercek-zamanli-tuketim.xhtml). The data set includes the electricity consumption in Turkey from 1st of January, 2016 till the 1st of December, 2020. After loading the data set into R, I have changed the column names from Turkish to English and simplified them. Date and hour columns are coerced into time classes from string. Finally, the electricity consumption column is coerced into numerical form from string data type.

```{r warning = FALSE, message = FALSE, cache = TRUE}
data <- read.csv("data.csv")
colnames(data) <- c("Date", "Hour", "Consumption")
data$Date <- dmy(data$Date)
data$Hour <- hm(data$Hour)
data$Hour <- data$Hour$hour
data$Consumption <- gsub(".", "", data$Consumption, fixed = T)
data$Consumption <- gsub(",", ".", data$Consumption, fixed = T)
data$Consumption <- as.numeric(data$Consumption)
```

After that, I have checked if there is a duplicate row because there might be problems with the data acquisition procedure.

```{r warning = FALSE, message = FALSE, cache = TRUE}
sum(duplicated(data))
```

The sum of the boolean values are 1, so there is a duplicated row in this data set. In order to find which row is duplicated, I have used the which() function. 2069th row is duplicated. I have filtered the data with the date of the duplicated part in order to fix it. I have changed the hour variable and made it's consumption value 0 in order to aviod confusions in the future tasks.

```{r warning = FALSE, message = FALSE, cache = TRUE}
which(duplicated(data))
data[2069,]
duplicate <- data[data$Date == "2016-03-27",]
duplicate
data$Hour[2068] <- 3
data$Consumption[2068] <- 0
data[data$Date == "2016-03-27",]
```

# Task 1

For the first task, we are using naive approach to predict future values of electricity consumption. 2-day lagged values and 7-day lagged values of the data set are added as columns. These lagged values will be our predictions of future consumption values. After adding the lagged value columns, I have splitted the data set into train and test sets, which is not with random sampling like usually done in the machine learning because the data set is a time series. Therefore, we have to select the test and train sets with consecutive dates. 1st November, 2020 is selected as cutoff point for train set.

```{r warning = FALSE, message = FALSE, cache = TRUE}
data <- data %>% mutate(lag48 = lag(Consumption, 48)) %>% mutate(lag168 = lag(Consumption, 168))
train <- data[data$Date < "2020-11-01",]
test <- data[data$Date >= "2020-11-01",]
MAPE(test$lag48, test$Consumption)
MAPE(test$lag168, test$Consumption)
```

MAPE metric for 2-day lagged values is 0.07789204 while MAPE metric for 7-day lagged values is 0.03453669. As a result, we can assume that weekly seasonality is stronger than the 2-day seasonality in the Turkey's electricity consumption.

# Task 2

In this task, I have built a multiple linear regression model with 2-day lagged values and 7-day lagged values as independent features. To achieve this, I have removed the observations with the NA values.

```{r warning = FALSE, message = FALSE, cache = TRUE}
train <- train[complete.cases(train),]
fit <- lm(Consumption ~ lag48 + lag168, data = train)
summary(fit)
predictions <- predict(fit, newdata = test)
MAPE(predictions, test$Consumption)
```

MAPE metric for the linear regression with the lagged values as input features is 0.04123266. This MAPE metric is worse than the naive approach with 7-day lagged values as predictions. Therefore, this model is not good enough to predict future values, because anyone with ability to count 7-day backwards can predict better than this in the long-run.

# Task 3

Until now, we have assumed the seasonality of electricity consumption is accounted with days or weeks. We did not think about hourly seasonality in a day. To test this idea, I have built a linear regression model for each hour.

```{r warning = FALSE, message = FALSE, cache = TRUE}
mape_hour <- data.frame(Hour = 0:23, MAPE = rep(NA, 24))
for(i in 0:23){
  fit_hour <- lm(Consumption ~ lag48 + lag168, data = train[train$Hour == i,])
  predictions_hour <- predict(fit_hour, newdata = test[test$Hour == i,])
  mape_hour$MAPE[i+1] <- MAPE(predictions_hour, test[test$Hour == i,]$Consumption)
}
mape_hour$MAPE
mape_hour[mape_hour$MAPE < MAPE(test$lag168, test$Consumption),]
```

We have found bunch of models with better results than naive 7-days lagged approach. In these models, we can clearly see that there are groupings of hours. Early morning hours and late evening hours are grouped together, and both groups have better MAPE metrics than 7-days lagged approach. Therefore, assuming that there are hourly seasonality in a day for electricity consumption is appropriate.

# Task 4

One of my friends came up with an alternative approach assuming that all hourly consumption values of last week (same day) can be important in the prediction of the next day’s consumption. To achieve this model, I have manipulated the data set for achieving the "wide" format.

```{r warning = FALSE, message = FALSE, cache = TRUE}
wide_48 <- data[, 1:3] %>% pivot_wider(names_from = Hour, values_from = Consumption, names_prefix = "lag_day2_")
wide_168 <- data[, 1:3] %>% pivot_wider(names_from = Hour, values_from = Consumption, names_prefix = "lag_day7_")
wide_48 <- wide_48[, -1] %>% mutate_all(funs(lag), n = 2)
wide_48$Date <- wide_168$Date
wide_48 <- wide_48[, c(25, 1:24)]
wide_168 <- wide_168[, -1] %>% mutate_all(funs(lag), n = 7)
wide_168$Date <- wide_48$Date
wide_168 <- wide_168[, c(25, 1:24)]
wide_all <- data[, 1:3] %>% inner_join(wide_48, by = "Date") %>% inner_join(wide_168, by = "Date")
wide_all <- wide_all[complete.cases(wide_all),]
```

I have followed the same logic with the task 3 and modeled each hour separately. Since there is an auto-correlation between 48 predictors (2-days lagged hours + 7-days lagged hours = 48 feature), I have used the penalized regression approach, Lasso Regression. In order to determine regularization parameter, I have performed 10-fold cross-validation, and modeled the each hour.

```{r warning = FALSE, message = FALSE, cache = TRUE}
train_lasso <- wide_all[wide_all$Date < "2020-11-01", ]
test_lasso <- wide_all[wide_all$Date >= "2020-11-01", ]
cv <- cv.glmnet(as.matrix(train_lasso[, -c(1,2,3)]), as.matrix(train_lasso[, 3]))
cv
min_lambda <- cv$lambda.min
lasso_mape <- data.frame(Hour = 0:23, MAPE = rep(NA, 24))
for(i in 0:23){
  lasso_model <- glmnet(as.matrix(train_lasso[train_lasso$Hour == i, 4:51]),
                        as.matrix(train_lasso[train_lasso$Hour == i, 3]), alpha = 1,
                        lambda = min_lambda)
  lasso_predictions <- predict(lasso_model, newx = as.matrix(test_lasso[test_lasso$Hour == i, 4:51]))
  lasso_mape$MAPE[i+1] <- MAPE(lasso_predictions, as.matrix(test_lasso[test_lasso$Hour == i, 3]))
}
lasso_mape$MAPE
```

As a result, I have obtained the best MAPE metrics throughout this homework. Early morning hours and late evening hours are the best performing models like in the task 3. This means weekly seasonality and hourly seasonality is present in the data set, therefore both weekly and hourly seasonality should be taken into account while taking care of the auto-correlation between input variables by modeling the electricity consumption of Turkey with penalized regression approaches.

# Task 6

I have created a boxplot of MAPE values of the evaluated approaches in this homework.

```{r warning = FALSE, message = FALSE, cache = TRUE}
hourly <- c()
for(i in 0:23){
  hourly[i+1] <- paste("Hour ", i, " Linear Regression", sep = "")
}
lasso_hourly <- c()
for(i in 0:23){
  lasso_hourly[i+1] <- paste("Hour ", i, " Lasso Regression", sep = "")
}
mape <- data.frame(MAPE = c(MAPE(test$lag48, test$Consumption), MAPE(test$lag168, test$Consumption),
                            MAPE(predictions, test$Consumption), mape_hour$MAPE, lasso_mape$MAPE),
                   Model = c("2 days Lag Naive", "7 days Lag Naive", "2 days + 7 days Linear Regression",
                             hourly, lasso_hourly ))
g <- ggplot(mape, aes(y = MAPE)) +
  geom_boxplot()
g <- ggplotly(g)
g <- div(g, align = "center")
g
```

Median of the MAPE values is 0.03 and 1st quantile of the MAPE values is 0.02, therefore I can state that models with MAPE values below 0.03 are considered as okay models but not good enough from business perspective. MAPE values below 0.02 are the top models of this homework. You can see the top 5 models of this homework below,

```{r warning = FALSE, message = FALSE, cache = TRUE}
mape_ordered <- mape[order(mape$MAPE),]
head(mape_ordered, 5)
```

